{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "quantization_aware_training_comprehensive guide.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbORZA_bQx1G",
        "colab_type": "text"
      },
      "source": [
        "Welcome to the comprehensive guide for Keras quantization-aware training.\n",
        "\n",
        "Use this page to quickly find the APIs you need for your use case via the navigation sidebar. Once you know which APIs you need, find the parameters and the low-level details in the\n",
        "[API docs](https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity). \n",
        "\n",
        "*  If you want to see the benefits of quantization-aware training and what's supported, see the [overview](https://www.tensorflow.org/model_optimization/guide/quantization/training.md). \n",
        "*  For a single end-to-end example, see the [quantization-aware training example](https://www.tensorflow.org/model_optimization/guide/quantization/quantization_aware_training_guide.md).\n",
        "\n",
        "The following corresponds to the navigation sidebar:\n",
        "\n",
        "You will either want to **deploy with quantization** or **research quantization**.\n",
        "* Without a quantization-aware model, you must **define** the model. Training\n",
        "  the model is standard Keras.\n",
        "* For Keras HDF5 models only, you need special **checkpointing and deserialization**.\n",
        "\n",
        "Run the boilerplate code below once to start."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuABqZnXVDvO",
        "colab_type": "text"
      },
      "source": [
        "# Boilerplate: run once per Colab session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvpH1Hg7ULFz",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "48d5004d-5f0f-46f9-baeb-dd59dac7e05d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Run this section once per Colab session.\n",
        "\n",
        "! pip uninstall -y tensorflow\n",
        "! pip install -q tf-nightly==2.2.0.dev20200305\n",
        "! pip install -q --extra-index-url=https://test.pypi.org/simple/ tensorflow-model-optimization==0.3.0.dev3\n",
        "\n",
        "import tempfile\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "import tempfile\n",
        "\n",
        "input_shape = [20]\n",
        "x_train = np.random.randn(1, 20).astype(np.float32)\n",
        "y_train = tf.keras.utils.to_categorical(np.random.randn(1), num_classes=20)\n",
        "\n",
        "def setup_model():\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(20, input_shape=input_shape),\n",
        "      tf.keras.layers.Flatten()\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "def setup_pretrained_weights():\n",
        "  model= setup_model()\n",
        "\n",
        "  model.compile(\n",
        "      loss=tf.keras.losses.categorical_crossentropy,\n",
        "      optimizer='adam',\n",
        "      metrics=['accuracy']\n",
        "  )\n",
        "    \n",
        "  model.fit(x_train, y_train)\n",
        "\n",
        "  _, pretrained_weights = tempfile.mkstemp('.h5')\n",
        "\n",
        "  model.save_weights(pretrained_weights)\n",
        "\n",
        "  return pretrained_weights\n",
        "\n",
        "def setup_pretrained_model():\n",
        "  model = setup_model()\n",
        "  pretrained_weights = setup_pretrained_weights()\n",
        "  model.load_weights(pretrained_weights)\n",
        "  return model\n",
        "  \n",
        "setup_model()\n",
        "pretrained_weights = setup_pretrained_weights()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.5983 - accuracy: 0.0000e+00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTHLMLV-ZrUA",
        "colab_type": "text"
      },
      "source": [
        "# Deploy with quantization with defaults"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0U6XAUhIe6re",
        "colab_type": "text"
      },
      "source": [
        "By creating models in the following fashion, there is an available path to deployment to backends listed in the [overview page](https://www.tensorflow.org/model_optimization/guide/quantization/training.md)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcPaZz5VZvMh",
        "colab_type": "text"
      },
      "source": [
        "## Define quantization-aware model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ybigft1fTn4T",
        "colab_type": "text"
      },
      "source": [
        "### Quantize all layers in Functional and Sequential models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puZvqnp1xsn-",
        "colab_type": "text"
      },
      "source": [
        "**Tips** for better model accuracy:\n",
        "\n",
        "* Try \"Quantize some layers\" on the navigation sidebar to skip quantizing the layers that reduce accuracy the most\n",
        "and focus on the ones that benefit latency the most.\n",
        "* Generally better to start from pre-trained weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIn-hFO_T_PU",
        "colab_type": "code",
        "outputId": "4626988b-aafb-49f5-fb9a-3fd8d6a0e394",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "model = setup_model()\n",
        "model.load_weights(pretrained_weights) # optional but recommended.\n",
        "\n",
        "quant_aware_model = tfmot.quantization.keras.quantize_model(model)\n",
        "\n",
        "quant_aware_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "quant_dense_4 (QuantizeWrapp (None, 20)                425       \n",
            "_________________________________________________________________\n",
            "quant_flatten_4 (QuantizeWra (None, 20)                1         \n",
            "=================================================================\n",
            "Total params: 426\n",
            "Trainable params: 420\n",
            "Non-trainable params: 6\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTbTLn3dZM7h",
        "colab_type": "text"
      },
      "source": [
        "### Quantize subset of layers in Functional and Sequential models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbM8o832xTxV",
        "colab_type": "text"
      },
      "source": [
        "During deployment, the non-quantized layers would execute in float. \n",
        "\n",
        "In the example below, we achieve the same result three ways: \n",
        "* quantizing all Dense layers\n",
        "* quantizing the only Dense layer\n",
        "* skipping quantization for the only non-Dense layer.\n",
        "\n",
        "**Tips** for better model accuracy:\n",
        "\n",
        "* Generally better to start from pre-trained weights.\n",
        "* Try quantizing the later layers instead of the first layers.\n",
        "* Avoid quantizing critical layers (e.g. attention mechanism). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HN0B_QB-ZhE2",
        "colab_type": "code",
        "outputId": "b26a3d50-548a-4028-a55f-b8d98735028b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        }
      },
      "source": [
        "model = setup_model()\n",
        "model.load_weights(pretrained_weights) # optional but recommended\n",
        "\n",
        "## Version 1: Quantize all dense layers.\n",
        "def apply_quantization_to_dense(layer):\n",
        "  if isinstance(layer, tf.keras.layers.Dense):\n",
        "    return tfmot.quantization.keras.quantize_annotate_layer(layer)\n",
        "  return layer\n",
        "    \n",
        "annotated_model = tf.keras.models.clone_model(\n",
        "    model, \n",
        "    clone_function=apply_quantization_to_dense,\n",
        ")\n",
        "\n",
        "quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
        "\n",
        "print(\"quantize_dense_layers\")\n",
        "quant_aware_model.summary()\n",
        "\n",
        "## Version 2: Quantize only the first layer.\n",
        "def layers_to_quantize():\n",
        "  # Knowing that the first layer is the Dense layer.\n",
        "  return {model.layers[0]: 'default'}\n",
        "\n",
        "def apply_quantization_to_first(layer):\n",
        "  if layer in layers_to_quantize():\n",
        "    return tfmot.quantization.keras.quantize_annotate_layer(layer)\n",
        "  return layer\n",
        "\n",
        "annotated_model_first_layer = tf.keras.models.clone_model(\n",
        "    model,\n",
        "    clone_function = apply_quantization_to_first\n",
        ")\n",
        "\n",
        "quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"quantize_first_layer\")\n",
        "quant_aware_model.summary()\n",
        "\n",
        "## Version 3: Skip quantizing only the last layer. \n",
        "def layers_to_skip():\n",
        "  # Knowing that the last layer is the only non-Dense layer.\n",
        "  return {model.layers[1]: 'skip'}\n",
        "\n",
        "def skip_quantizing_one_layer(layer):\n",
        "  if layer in layers_to_skip():\n",
        "    return layer\n",
        "  return tfmot.quantization.keras.quantize_annotate_layer(layer)\n",
        "\n",
        "annotated_model_first_layer = tf.keras.models.clone_model(\n",
        "    model,\n",
        "    clone_function = skip_quantizing_one_layer,\n",
        ")\n",
        "\n",
        "quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"skip_quantizing_one_layer\")\n",
        "quant_aware_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "quantize_dense_layers\n",
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "quant_dense_16 (QuantizeWrap (None, 20)                425       \n",
            "_________________________________________________________________\n",
            "flatten_15 (Flatten)         (None, 20)                0         \n",
            "=================================================================\n",
            "Total params: 425\n",
            "Trainable params: 420\n",
            "Non-trainable params: 5\n",
            "_________________________________________________________________\n",
            "\n",
            "\n",
            "quantize_first_layer\n",
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "quant_dense_16 (QuantizeWrap (None, 20)                425       \n",
            "_________________________________________________________________\n",
            "flatten_15 (Flatten)         (None, 20)                0         \n",
            "=================================================================\n",
            "Total params: 425\n",
            "Trainable params: 420\n",
            "Non-trainable params: 5\n",
            "_________________________________________________________________\n",
            "\n",
            "\n",
            "skip_quantizing_one_layer\n",
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "quant_dense_16 (QuantizeWrap (None, 20)                425       \n",
            "_________________________________________________________________\n",
            "flatten_15 (Flatten)         (None, 20)                0         \n",
            "=================================================================\n",
            "Total params: 425\n",
            "Trainable params: 420\n",
            "Non-trainable params: 5\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpb_BydRaSoF",
        "colab_type": "text"
      },
      "source": [
        "#### More readable but potentially lower model accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vqXeYffzSHp",
        "colab_type": "text"
      },
      "source": [
        "This is not compatible with using pre-trained weights, which is why it may be less accurate than the above examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQoMH3g3fWwb",
        "colab_type": "text"
      },
      "source": [
        "**Functional example**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Wow55hg5oiM",
        "colab_type": "code",
        "outputId": "6ac062ed-1bcf-4df8-a2e3-ecc7db96d40e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "i = tf.keras.Input(shape=(20,))\n",
        "x = tfmot.quantization.keras.quantize_annotate_layer(tf.keras.layers.Dense(10))(i)\n",
        "o = tf.keras.layers.Flatten()(x)\n",
        "\n",
        "annotated_model = tf.keras.Model(inputs=i, outputs=o)\n",
        "quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
        "\n",
        "# For deployment, the tool adds `QuantizeLayer` after `InputLayer` so that the \n",
        "# quantized model can take in float inputs instead of only uint8.\n",
        "quant_aware_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 20)]              0         \n",
            "_________________________________________________________________\n",
            "quantize_layer (QuantizeLaye (None, 20)                3         \n",
            "_________________________________________________________________\n",
            "quant_dense_6 (QuantizeWrapp (None, 10)                215       \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 218\n",
            "Trainable params: 210\n",
            "Non-trainable params: 8\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIGj-r2of2ls",
        "colab_type": "text"
      },
      "source": [
        "**Sequential example**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQOiDUGgfi4y",
        "colab_type": "code",
        "outputId": "05323571-5d61-4109-a617-df18cc21ec63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "annotated_model = tf.keras.Sequential([\n",
        "  tfmot.quantization.keras.quantize_annotate_layer(tf.keras.layers.Dense(20, input_shape=input_shape)),\n",
        "  tf.keras.layers.Flatten()\n",
        "])\n",
        "\n",
        "quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
        "\n",
        "quant_aware_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "quant_dense_7 (QuantizeWrapp (None, 20)                425       \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 20)                0         \n",
            "=================================================================\n",
            "Total params: 425\n",
            "Trainable params: 420\n",
            "Non-trainable params: 5\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeNCMDAbnEKU",
        "colab_type": "text"
      },
      "source": [
        "## Create quantized model and deploy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiYk_KR0rJ2n",
        "colab_type": "text"
      },
      "source": [
        "See the documentation from the deployment backend that you are interested in using. \n",
        "\n",
        "As an example, this is how it's done for TFLite. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbBiEetda3R8",
        "colab_type": "code",
        "outputId": "4f0aa8ba-c10e-4e34-ac82-729d7c2fd7e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model = setup_pretrained_model()\n",
        "quant_aware_model = tfmot.quantization.keras.quantize_model(model)\n",
        "\n",
        "# Typically you finetune the model first. \n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "quantized_tflite_model = converter.convert()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 2ms/step - loss: 5.1110 - accuracy: 0.0000e+00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5raSy9ghxkv",
        "colab_type": "text"
      },
      "source": [
        "# Research quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUGpXIET0cy3",
        "colab_type": "text"
      },
      "source": [
        "For understanding, we recommend first reading the simpler\n",
        "\"Deploy with quantization with defaults\" section.\n",
        "\n",
        "There is no supported path to deployment here from defining the models\n",
        "in the following ways."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHxdS_sCpkwS",
        "colab_type": "text"
      },
      "source": [
        "## Define model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnMguvVSnUqD",
        "colab_type": "text"
      },
      "source": [
        "## Modify quantization parameters or parts of layer to quantize, or quantize custom Keras layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLgH1aFMjTK4",
        "colab_type": "text"
      },
      "source": [
        "This example modifies the default quantization implementation for Dense. \n",
        "\n",
        "The weights now use 4-bits instead of 8-bits and the activation is no longer quantized. The rest of the model continues to use the default quantization implementation.\n",
        "\n",
        "**Your use case**: handling custom Keras layers uses the same `QuantizeConfig` interface used to modify quantization for the built-in Dense layer.\n",
        "\n",
        "**Common mistake:** quantizing the bias to fewer to 32-bits usually harms model accuracy too much.\n",
        "\n",
        "TODO: change to usage pattern from \"Quantize subset of layers\" that supports pre-trained models while linking to \"More readable ...\" section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77jgBjccnTh6",
        "colab_type": "code",
        "outputId": "e79f097a-d68e-4101-fccd-23655af75d7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "quantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer\n",
        "quantize_annotate_model = tfmot.quantization.keras.quantize_annotate_model\n",
        "\n",
        "# TODO: change to .quantizers.\n",
        "LastValueQuantizer = tfmot.quantization.keras.LastValueQuantizer\n",
        "QuantizeConfig = tfmot.quantization.keras.QuantizeConfig\n",
        "\n",
        "custom_object_scope = tf.keras.utils.custom_object_scope\n",
        "\n",
        "\n",
        "# TODO: should make this num_bits change simpler. Proposal on simple\n",
        "# changes we can do for this.\n",
        "class DenseQuantizeConfig(QuantizeConfig):\n",
        "    \"\"\"Custom QuantizeConfig for Dense layer.\n",
        "\n",
        "    The QuantizeConfig allows you to precisely choose,\n",
        "      a. what to quantize in a layer and\n",
        "      b. how to quantize it via the Quantizer\n",
        "    \"\"\"\n",
        "\n",
        "    def get_weights_and_quantizers(self, layer):\n",
        "      # Use 4-bits to quantized weights instead of 8.\n",
        "      # TODO: make per_axis=True supported for LastValueQuantizer. \n",
        "      return [(layer.kernel, LastValueQuantizer(num_bits=4, symmetric=True, narrow_range=False, per_axis=False))]\n",
        "    \n",
        "    def get_activations_and_quantizers(self, layer):\n",
        "      # Don't quantize the activation.\n",
        "      return []\n",
        "\n",
        "    def set_quantize_weights(self, layer, quantize_weights):\n",
        "      layer.kernel = quantize_weights[0]\n",
        "\n",
        "    def set_quantize_activations(self, layer, quantize_activations):\n",
        "      return\n",
        "\n",
        "    def get_output_quantizers(self, layer):\n",
        "      return []\n",
        "\n",
        "    def get_config(self): \n",
        "      return {}\n",
        "\n",
        "model = quantize_annotate_model(tf.keras.Sequential([\n",
        "   # Quantize Dense with custom implementation.\n",
        "   quantize_annotate_layer(tf.keras.layers.Dense(20, input_shape=(20,)), DenseQuantizeConfig()),\n",
        "   # Other layers use default quantization implementation via `quantize_annotate_model`\n",
        "   tf.keras.layers.Flatten()\n",
        "]))\n",
        "\n",
        "with custom_object_scope({\n",
        "      'DenseQuantizeConfig': DenseQuantizeConfig\n",
        "}):\n",
        "  quant_aware_model = tfmot.quantization.keras.quantize_apply(model)\n",
        "\n",
        "quant_aware_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "quant_dense_12 (QuantizeWrap (None, 20)                423       \n",
            "_________________________________________________________________\n",
            "quant_flatten_11 (QuantizeWr (None, 20)                1         \n",
            "=================================================================\n",
            "Total params: 424\n",
            "Trainable params: 420\n",
            "Non-trainable params: 4\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yD0sIR6tmmRx",
        "colab_type": "text"
      },
      "source": [
        "## Use custom quantization algorithm\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4onhF-H1zsn",
        "colab_type": "text"
      },
      "source": [
        "For how to use `QuantizeConfig`, see the \"Modify quantization parameters or parts of layer to quantize ...\" section on the navigation sidebar, which this\n",
        "increased flexibility is an extension of.\n",
        "\n",
        "TODO: fix example. Our wrapper logic is hard coded to handle\n",
        "only LastValueQuantizer and MovingAverageQuantizer and doesn't support\n",
        "example below yet. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jt8UioZH49QV",
        "colab_type": "code",
        "outputId": "81742fb0-01d1-4855-d041-de5443183cc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "# TODO: should still keep as Quantizer instead of quantizers.Quantizer.\n",
        "Quantizer = tfmot.quantization.keras.Quantizer\n",
        "QuantizeConfig = tfmot.quantization.keras.QuantizeConfig\n",
        "\n",
        "class FixedRangeQuantizer(Quantizer):\n",
        "  \"\"\"Quantizer which keeps values between -1 and 1.\"\"\"\n",
        "\n",
        "  def build(self, tensor_shape, name, layer):\n",
        "    # Not needed. No TensorFlow variables. \n",
        "    return\n",
        "\n",
        "  def __call__(self, inputs, step, training, **kwargs):\n",
        "    return tf.keras.backend.clip(inputs, -1.0, 1.0)\n",
        "\n",
        "  def get_config():\n",
        "    # Not needed. No __init__ parameters to serialize.\n",
        "    return {}\n",
        "\n",
        "\n",
        "# This custom Quantizer can now be used in a QuantizeConfig as specified above.\n",
        "class DenseQuantizeConfig(QuantizeConfig):\n",
        "    \"\"\"Custom QuantizeConfig for Conv layer.\"\"\"\n",
        "\n",
        "    def get_weights_and_quantizers(self, layer):\n",
        "      # Use FixedRangeQuantizer instead of default Quantizer.\n",
        "      return [(layer.kernel, FixedRangeQuantizer())]\n",
        "    \n",
        "    def get_activations_and_quantizers(self, layer):\n",
        "      # Keep defaults here.\n",
        "      return [(layer.activation, MovingAverageQuantizer(num_bits=8, per_axis=False, symmetric=False, narrow_range=False))]\n",
        "\n",
        "    def set_quantize_weights(self, layer, quantize_weights):\n",
        "      layer.kernel = quantize_weights[0]\n",
        "\n",
        "    def set_quantize_activations(self, layer, quantize_activations):\n",
        "      return\n",
        "\n",
        "    def get_output_quantizers(self, layer):\n",
        "      return []\n",
        "\n",
        "    def get_config(self): \n",
        "      return {}\n",
        "\n",
        "model = quantize_annotate_model(tf.keras.Sequential([\n",
        "   # Quantize Dense with custom implementation.\n",
        "   quantize_annotate_layer(tf.keras.layers.Dense(20, input_shape=(20,)), DenseQuantizeConfig()),\n",
        "   # Other layers use default quantization implementation via `quantize_annotate_model`\n",
        "   tf.keras.layers.Flatten()\n",
        "]))\n",
        "\n",
        "with custom_object_scope({\n",
        "      'DenseQuantizeConfig': DenseQuantizeConfig\n",
        "}):\n",
        "  quant_aware_model = tfmot.quantization.keras.quantize_apply(model)\n",
        "\n",
        "# TODO: test this with training also."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-fff92b1c8ba2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m       \u001b[0;34m'DenseQuantizeConfig'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDenseQuantizeConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m }):\n\u001b[0;32m---> 53\u001b[0;31m   \u001b[0mquant_aware_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfmot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantize_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize.py\u001b[0m in \u001b[0;36mquantize_apply\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m   return keras.models.clone_model(\n\u001b[0;32m--> 316\u001b[0;31m       transformed_model, input_tensors=None, clone_function=_quantize)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/models.py\u001b[0m in \u001b[0;36mclone_model\u001b[0;34m(model, input_tensors, clone_function)\u001b[0m\n\u001b[1;32m    422\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     return _clone_sequential_model(\n\u001b[0;32m--> 424\u001b[0;31m         model, input_tensors=input_tensors, layer_fn=clone_function)\n\u001b[0m\u001b[1;32m    425\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m     return _clone_functional_model(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/models.py\u001b[0m in \u001b[0;36m_clone_sequential_model\u001b[0;34m(model, input_tensors, layer_fn)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minput_tensors\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m     \u001b[0mcloned_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneric_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     raise ValueError('To clone a `Sequential` model, we expect '\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, layers, name)\u001b[0m\n\u001b[1;32m    127\u001b[0m       \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_no_legacy_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    211\u001b[0m       \u001b[0;31m# If the model is being built continuously on top of an input layer:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m       \u001b[0;31m# refresh its output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m       \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSINGLE_LAYER_OUTPUT_ERROR_MSG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    894\u001b[0m           \u001b[0;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 896\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    897\u001b[0m           \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2401\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2402\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2403\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2404\u001b[0m       \u001b[0;31m# We must set also ensure that the layer is marked as built, and the build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2405\u001b[0m       \u001b[0;31m# shape is stored since user defined build functions may not be calling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize_wrapper.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantize_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights_and_quantizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m       min_var, max_var = quantizer.build(\n\u001b[0;32m---> 94\u001b[0;31m           weight.shape, self._weight_name(weight.name), self)\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_weight_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpvX5IqahV1r",
        "colab_type": "text"
      },
      "source": [
        "# Checkpointing and Deserialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuZ5wlij1dcJ",
        "colab_type": "text"
      },
      "source": [
        "**Your Use Case:** this code is only needed for the HDF5 model format (not HDF5 weights or other formats)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6khQg-q7imfH",
        "colab_type": "code",
        "outputId": "ad7f7f32-ebf8-4260-d15b-0ff201694762",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "# See \"Define model\" on navigation sidebar for \n",
        "# how to define this model in other ways.\n",
        "model = setup_model()\n",
        "model.load_weights(pretrained_weights) # optional but recommended.\n",
        "quant_aware_model = tfmot.quantization.keras.quantize_model(model)\n",
        "\n",
        "_, keras_model_file = tempfile.mkstemp('.h5')\n",
        "\n",
        "quant_aware_model.save(keras_model_file)\n",
        "\n",
        "with tfmot.quantization.keras.quantize_scope():\n",
        "  loaded_model = tf.keras.models.load_model(keras_model_file)\n",
        "\n",
        "loaded_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "quant_dense_13 (QuantizeWrap (None, 20)                425       \n",
            "_________________________________________________________________\n",
            "quant_flatten_12 (QuantizeWr (None, 20)                1         \n",
            "=================================================================\n",
            "Total params: 426\n",
            "Trainable params: 420\n",
            "Non-trainable params: 6\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}